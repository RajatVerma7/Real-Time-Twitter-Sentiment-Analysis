{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ptvPCIJensN"
      },
      "source": [
        "# Importing Libraries and Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jH-GI6egem"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
        "import re, string, random\n",
        "import io\n",
        "import collections\n",
        "from nltk.metrics import *\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_vV6qm8es5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d77b538-9312-45ce-e50e-954c8478b7a1"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nacBxmXezgi"
      },
      "source": [
        "## Importing Dataset from My Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgFmUppZfCpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f770d2-20b3-4e07-9e74-9317585f6401"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6es-lMOfX3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c56eefd-8d88-4672-a99e-b9281292506c"
      },
      "source": [
        "#Storing dataset into a variable 'df'\n",
        "df = pd.read_csv(('/content/drive/My Drive/Twitter Dataset/Twitter.csv'),encoding='cp1252',names=['target','ids','date,time','flag','user','text'])\n",
        "df.shape"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sY0RKZ_gdCc"
      },
      "source": [
        "#Taking out Tweets and Target out from dataset 'df'\n",
        "tweets = df['text']\n",
        "target=df['target']\n"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnM4ffwM7lP9",
        "outputId": "616fe30f-2863-4931-ce51-77838bf8d59e"
      },
      "source": [
        "tweets"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          is upset that he can't update his Facebook by ...\n",
              "2          @Kenichan I dived many times for the ball. Man...\n",
              "3            my whole body feels itchy and like its on fire \n",
              "4          @nationwideclass no, it's not behaving at all....\n",
              "                                 ...                        \n",
              "1599995    Just woke up. Having no school is the best fee...\n",
              "1599996    TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997    Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998    Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999    happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "Name: text, Length: 1600000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ZXh1qZ7-Tm"
      },
      "source": [
        "#Converting Pandas series to Numpy array\n",
        "tweets = df['text'].to_numpy()\n",
        "target=df['target'].to_numpy()"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foUOFLXM8M1Z",
        "outputId": "b134ff6e-220d-4d80-cda6-6fe0aad46840"
      },
      "source": [
        "tweets"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\",\n",
              "       \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\",\n",
              "       '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds',\n",
              "       ..., 'Are you ready for your MoJo Makeover? Ask me for details ',\n",
              "       'Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur ',\n",
              "       'happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep_pnKfJghxm"
      },
      "source": [
        "#Shuffling data\n",
        "from sklearn.utils import shuffle\n",
        "tweets,target= shuffle(tweets,target, random_state=10)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yxI0dQxhY6q"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEmzHCtuhxWI"
      },
      "source": [
        "#Function to remove stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(string):\n",
        "        new=string.lower().split()\n",
        "        new = [word for word in new if not word in stopwords]\n",
        "        new = ' '.join(new) \n",
        "        return new"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8HH3wHWkEDM"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rZylHrvhzJw"
      },
      "source": [
        "#Function to perform Stemming\n",
        "ps = PorterStemmer()\n",
        "def stem(string):\n",
        "        new=string.lower().split()\n",
        "        new = [ps.stem(word) for word in new]\n",
        "        new = ' '.join(new)\n",
        "        return new"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDGv55IXkGzV"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txhHIeIGi504"
      },
      "source": [
        "#Function to perform Lemmatization on a Corpus\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(tweets):\n",
        "\n",
        "    def get_wordnet_pos(word):\n",
        "        # Map POS tag to first character lemmatize() accepts\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    lemmatized_tweets=[]\n",
        "\n",
        "    for i in tweets:\n",
        "        tweet = i\n",
        "        tweet = tweet.lower()\n",
        "        tweet = tweet.split()\n",
        "        tweet = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tweet]\n",
        "        tweet = ' '.join(tweet) \n",
        "        lemmatized_tweets.append(tweet)\n",
        "    return(lemmatized_tweets)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22L2Bdn3u85x"
      },
      "source": [
        "Removing Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVD5KfGZqJC3"
      },
      "source": [
        "def remove_noise(tweets):\n",
        "    cleaned_tweets = []\n",
        "    for i in tweets:\n",
        "        tweet=i\n",
        "        tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                    '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',tweet)\n",
        "        tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\",tweet)\n",
        "        cleaned_tweets.append(tweet)\n",
        "    return (cleaned_tweets)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg1LyYbpv3Rj"
      },
      "source": [
        "Tokenization of corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMtiyDk0veN7"
      },
      "source": [
        "def tokenize(tweets):\n",
        "    tokenized_cleaned_tweets = [(tweet.split()) for tweet in tweets]\n",
        "    return (tokenized_cleaned_tweets)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tp3X4rFwk_3"
      },
      "source": [
        "## PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtWwoE2Yzs6V"
      },
      "source": [
        "cleaned_tweets=[]\n",
        "size=800000\n",
        "tweets=tweets[0:size]\n",
        "target=target[0:size]\n",
        "#Removing stopwords and permorming stemming\n",
        "for i in tweets:\n",
        "  st=remove_stopwords(i)\n",
        "  rs=stem(st)\n",
        "  cleaned_tweets.append(rs)\n",
        "l=lemmatize(cleaned_tweets)  #Lemmatization\n",
        "corpus=tokenize(remove_noise(l)) #Removing noise and tokenizing the corpus"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fkTCf6pr95V",
        "outputId": "b2036034-9c15-45d2-941a-da50574a1e29"
      },
      "source": [
        "len(tweets)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHYK2jT178Vx"
      },
      "source": [
        "## Preparation of Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D9jsis77-p1"
      },
      "source": [
        "def create_dictionary(tokenized_cleaned_corpus,target):\n",
        "    def get_tweets_for_model(cleaned_tokens_list):\n",
        "        for tweet_tokens in cleaned_tokens_list:\n",
        "            yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "    tokens_for_model = get_tweets_for_model(tokenized_cleaned_corpus)\n",
        "\n",
        "    final_corpus=[]\n",
        "    j=0\n",
        "    for i in tokens_for_model:\n",
        "        final_corpus.append((i, \"Positive\" if target[j]==4 else \"Negative\"))\n",
        "        j+=1\n",
        "\n",
        "    return(final_corpus)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_9DSAYk90Xc"
      },
      "source": [
        "final_corpus=create_dictionary(corpus,target)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ4WKQAj99Og",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c399f14-e5f1-4495-b357-001b81fa7a16"
      },
      "source": [
        "final_corpus[25000]"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'chair': True,\n",
              "  'goober!': True,\n",
              "  'love': True,\n",
              "  'push': True,\n",
              "  'stage': True,\n",
              "  'u': True,\n",
              "  'ur': True,\n",
              "  'wheel': True,\n",
              "  'ya': True},\n",
              " 'Positive')"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ZFw9Duqy5c",
        "outputId": "2eaef7fb-1330-4256-8adb-2749215b98e3"
      },
      "source": [
        "len(final_corpus)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEibtDgL-qEE"
      },
      "source": [
        "Training and Testing using Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb7LiWkn-sv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570f7c52-29c8-41dd-e4e6-281d7133ed6d"
      },
      "source": [
        "train_data = final_corpus[:int(size*0.75)] #75% for training\n",
        "test_data = final_corpus[int(size*0.75):] #25% for testing\n",
        "print(\"Size of Training data is \",len(train_data))\n",
        "print(\"Size of Testing data is \",len(test_data))"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Training data is  600000\n",
            "Size of Testing data is  200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgGtBistIop8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3ad051-7b5a-452a-a1a7-baa21fecd7fa"
      },
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                  farrah = True           Negati : Positi =     49.6 : 1.0\n",
            "                  sadden = True           Negati : Positi =     44.2 : 1.0\n",
            "                 *cries* = True           Negati : Positi =     40.4 : 1.0\n",
            "             heartbroken = True           Negati : Positi =     37.7 : 1.0\n",
            "                  condol = True           Negati : Positi =     37.0 : 1.0\n",
            "                   died! = True           Negati : Positi =     31.3 : 1.0\n",
            "                 nauseou = True           Negati : Positi =     27.0 : 1.0\n",
            "               heartburn = True           Negati : Positi =     25.0 : 1.0\n",
            "              headache.. = True           Negati : Positi =     24.4 : 1.0\n",
            "                  devast = True           Negati : Positi =     24.2 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbStMJH-DkVH"
      },
      "source": [
        "##Accuracy, Confusion Matrix, Precision, Recall, F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORyQnBDCOF_e",
        "outputId": "9828f76c-ae4c-4a26-f0bb-5c7eeec26b23"
      },
      "source": [
        "new_test_data=corpus[int(size*0.75):]\n",
        "new_test_data[0]\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['prepar', 'nice', 'websit']"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgPcyHIIDmro"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "pred=classifier.classify_many((dict([token, True] for token in i)) for i in new_test_data)\n",
        "true=[label[-1] for label in test_data]\n",
        "matrix=ConfusionMatrix(true,pred)\n",
        "\n",
        "y_pred=np.array(pred)\n",
        "y_true=np.array(true)\n",
        "ans=precision_recall_fscore_support(y_true, y_pred, average='macro')"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVwSGFxeGpzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517dd781-2117-4622-9d61-d84edda1cc09"
      },
      "source": [
        "print(\"Accuracy :\", (classify.accuracy(classifier, test_data))*100,\"%\")\n",
        "\n",
        "print(\"Confusion Matrix :\")\n",
        "print(matrix)\n",
        "\n",
        "print(\"Precision : \",ans[0])\n",
        "print(\"Recall : \",ans[1])\n",
        "print(\"F1 score : \",ans[2])"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 75.3465 %\n",
            "Confusion Matrix :\n",
            "         |     N     P |\n",
            "         |     e     o |\n",
            "         |     g     s |\n",
            "         |     a     i |\n",
            "         |     t     t |\n",
            "         |     i     i |\n",
            "         |     v     v |\n",
            "         |     e     e |\n",
            "---------+-------------+\n",
            "Negative |<81767>18297 |\n",
            "Positive | 31010<68926>|\n",
            "---------+-------------+\n",
            "(row = reference; col = test)\n",
            "\n",
            "Precision :  0.7576299673613962\n",
            "Recall :  0.7534242174025594\n",
            "F1 score :  0.7524445064387022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81QoSXndUjo0"
      },
      "source": [
        "import tweepy"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq9_weJJ1uzx",
        "outputId": "7364bfc2-0a32-4e0a-c099-00c22d481187"
      },
      "source": [
        "%run ./keys.ipynb"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:File `'./keys.ipynb.py'` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoILgSAg2OLc"
      },
      "source": [
        "consumer_key=\"ZK0X8p8TeX111DcmPQ7pkceAi\"\n",
        "consumer_secret=\"S1sIIER1K4c0chfSOV2Fv91xAtfdWF7RY2HpDbxCkYVsFgBSkB\"\n",
        "access_token=\"1137384737872535552-WAFL94Gg5vzeib9zbaYgvyz7IQAUkk\"\n",
        "access_token_secret=\"ipgS3MPSlacN7L1pYs6V2aEAPqOveFnUKYqpMPhGKv7Dc\""
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn3PayXX1131"
      },
      "source": [
        "auth=tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api=tweepy.API(auth)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DVaNxqD2aJN"
      },
      "source": [
        "number_of_tweets=10\n",
        "t=[]\n",
        "likes=[]\n",
        "for i in tweepy.Cursor(api.user_timeline, id=\"sachin_srt\", tweet_mode=\"extended\").items(number_of_tweets):\n",
        "  t.append(i.full_text)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOUeEunq6_48"
      },
      "source": [
        "t=np.array(t)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5L4QrCs8KIH"
      },
      "source": [
        "cleaned_tweets_x=[]\n",
        "for i in t:\n",
        "  st_x=remove_stopwords(i)\n",
        "  rs_x=stem(st_x)\n",
        "  cleaned_tweets_x.append(rs_x)\n",
        "l_x=lemmatize(cleaned_tweets_x)  #Lemmatization\n",
        "corpus_x=tokenize(remove_noise(l_x))"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EdPpjr0Nwt7",
        "outputId": "6c713284-9336-4d26-9f55-e5b03bed7af7"
      },
      "source": [
        "corpus_x"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['look',\n",
              "  'match',\n",
              "  'ind',\n",
              "  'v',\n",
              "  'pak..aft',\n",
              "  'long',\n",
              "  'time',\n",
              "  'match',\n",
              "  '2',\n",
              "  'countries...i',\n",
              "  'hope',\n",
              "  'india',\n",
              "  'rock',\n",
              "  'again....'],\n",
              " ['gud', '9t', 'all..'],\n",
              " ['hope',\n",
              "  'like',\n",
              "  'feel',\n",
              "  'one',\n",
              "  'year',\n",
              "  'babi',\n",
              "  'u',\n",
              "  'thrown',\n",
              "  'up...h',\n",
              "  'laugh',\n",
              "  \"b'coz\",\n",
              "  'know',\n",
              "  'u',\n",
              "  'catch',\n",
              "  'him...thi',\n",
              "  'hope!!'],\n",
              " ['thought', 'day-how', 'take', 'noth', 'high', 'fli'],\n",
              " ['sehwag&gambhir',\n",
              "  'drive',\n",
              "  'india',\n",
              "  'home',\n",
              "  'comfortable..viru',\n",
              "  'bowl',\n",
              "  'well',\n",
              "  'gauti',\n",
              "  'do',\n",
              "  'great',\n",
              "  'job',\n",
              "  'batting....but',\n",
              "  'miss',\n",
              "  'thunder',\n",
              "  'viru!!!'],\n",
              " ['gud',\n",
              "  '9t',\n",
              "  'all..in',\n",
              "  \"today'\",\n",
              "  'match',\n",
              "  \"afridi'\",\n",
              "  'great',\n",
              "  'knock',\n",
              "  'doesnt',\n",
              "  'work',\n",
              "  'pak',\n",
              "  'loo',\n",
              "  '16',\n",
              "  'runs..',\n",
              "  'malinga',\n",
              "  'bowl',\n",
              "  'well',\n",
              "  'today'],\n",
              " ['news',\n",
              "  'frm',\n",
              "  'away',\n",
              "  'holiday',\n",
              "  'kids.wont',\n",
              "  'get',\n",
              "  'holiday',\n",
              "  'next',\n",
              "  'year'],\n",
              " ['india',\n",
              "  'win',\n",
              "  'asia',\n",
              "  'cup.....our',\n",
              "  'side',\n",
              "  'look',\n",
              "  'good',\n",
              "  'n',\n",
              "  'balanc',\n",
              "  'bat',\n",
              "  'n',\n",
              "  'bowling.....al',\n",
              "  'best',\n",
              "  'team',\n",
              "  'india',\n",
              "  'make',\n",
              "  'u',\n",
              "  'proud!!'],\n",
              " ['hi', 'samalasam', 'thank', 'ur', 'comment'],\n",
              " ['&lt;a',\n",
              "  'href=\"\"',\n",
              "  'title=\"with',\n",
              "  'childhood',\n",
              "  'buddi',\n",
              "  'atul',\n",
              "  'ranad',\n",
              "  'twitpic']]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2CMc86uM-3r"
      },
      "source": [
        "ans=[]\n",
        "for i in range(0,10):\n",
        "  tesst= dict([token, True] for token in corpus_x[i])\n",
        "  pred_x=classifier.classify(tesst)\n",
        "  ans.append(pred_x)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BNdB-3gNE_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dddc67-62f1-45e9-cc85-2357a0b62476"
      },
      "source": [
        "t"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Looking for the match between ind vs pak..after a long time the match between these 2 countries...i hope india to rock again....',\n",
              "       'Gud 9t all..',\n",
              "       \"Hope is like a feeling of a one year baby when u thrown him up...he laughs b'coz he knowns u will catch him...this is the hope!!\",\n",
              "       'Thought for the day-how you take off has nothing to do with how high you will fly',\n",
              "       'Sehwag&gambhir drives india home comfortable..viru bowled well and gauti done great job in batting....but we miss the thunders from viru!!!',\n",
              "       \"Gud 9t all..in today's match afridi's great knock doesnt work for pak they loose by 16 runs.. Malinga bowled well today\",\n",
              "       'No news frm me for a while as i m away on holiday with my kids.wont get a holiday with them in next year',\n",
              "       'India will win the asia cup.....our side looks good n balancing in both batting n bowling.....all the best team india make us proud!!',\n",
              "       '@samalasam hi samalasam thanks for ur comments',\n",
              "       '&lt;a href=\"http://twitpic.com/1l136u\" title=\"With my childhood buddy Atul Ranade on Twitpic'],\n",
              "      dtype='<U139')"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b9spxahPIv7",
        "outputId": "51c1abf6-1d0b-4ad1-c616-9612612dbe67"
      },
      "source": [
        "ans"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Negative',\n",
              " 'Positive',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Negative',\n",
              " 'Positive',\n",
              " 'Positive',\n",
              " 'Negative']"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyTuBLR6TMpK"
      },
      "source": [
        "Some samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMgAR5thSfVq"
      },
      "source": [
        "def prediction(s):\n",
        "  cleaned=[]\n",
        "  st_res=remove_stopwords(s)\n",
        "  rs_res=stem(st_res)\n",
        "  cleaned.append(rs_res)\n",
        "  l_res=lemmatize(cleaned)  #Lemmatization\n",
        "  corpus_res=tokenize(remove_noise(l_res))\n",
        "  test= dict([token, True] for token in corpus_res[0])\n",
        "  pred_x=classifier.classify(test)\n",
        "  return pred_x"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "SJoTBHAOTeBf",
        "outputId": "fbb5bed8-a3c7-45d8-8400-6d15b41e6b7e"
      },
      "source": [
        "res=input(\"Enter a sentence! \\n\")\n",
        "prediction(res)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence! \n",
            "I like beautiful sky\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positive'"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "l7svjD1qSLmc",
        "outputId": "db6aa725-0729-46e6-8c3c-9ca6e6deacd1"
      },
      "source": [
        "res=input(\"Enter a sentence! \\n\")\n",
        "prediction(res)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence! \n",
            "I hate it when you are near me\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sulCYQ3zgNYv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}